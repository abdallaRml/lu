{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Copy of Copy of Step_3_6_BeginningToScrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdallaRml/lu/blob/master/Copy_of_Copy_of_Step_3_6_BeginningToScrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtROFYGd-pl6",
        "colab_type": "text"
      },
      "source": [
        "# Web Scraper\n",
        "in this this example we’ll cover how to format and interpret data without the help of a browser.\n",
        "This example starts with the basics of sending a GET request (a request to fetch, or “get,” the content of a web page) to a web server for a specific page, reading the HTML output from that page, and doing some simple data extraction in order to isolate the content that you are looking for.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYACPJm7_hZw",
        "colab_type": "text"
      },
      "source": [
        "# Connecting\n",
        "A web browser can tell the processor to send data to the application that handles your wireless (or wired) interface, but you can do the same thing in Python with just three lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w4Afa8Z2llE",
        "colab_type": "code",
        "outputId": "c8119a21-eae7-463a-e584-451e425fdfb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
        "print(html.read())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tdZbJiBcMy",
        "colab_type": "text"
      },
      "source": [
        "This command outputs the complete HTML code for page1 located at the URL http:// pythonscraping.com/pages/page1.html. \n",
        "\n",
        "More accurately, this outputs the HTML file page1.html, found in the directory <web root>/pages, on the server located at the domain name http://pythonscraping.com.\n",
        "Why is it important to start thinking of these addresses as “files” rather than “pages”? Most modern web pages have many resource files associated with them. These could be image files, JavaScript files, CSS files, or any other content that the page you are requesting is linked to. When a web browser hits a tag such as < img src=\"cuteKit ten.jpg\" >, the browser knows that it needs to make another request to the server to get the data at the file cuteKitten.jpg in order to fully render the page for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbdqTixeCraD",
        "colab_type": "text"
      },
      "source": [
        "# Running BeautifulSoup\n",
        "The most commonly used object in the BeautifulSoup library is, appropriately, the BeautifulSoup object.\n",
        "Note that this returns only the first instance of the h1 tag found on the page. By convention, only one h1 tag should be used on a single page, but conventions are often broken on the web, so you should be aware that this will retrieve the first instance of the tag only, and not necessarily the one that you’re looking for.\n",
        "As in previous web scraping examples, you are importing the urlopen function and calling html.read() in order to get the HTML content of the page. In addition to the text string, BeautifulSoup can also use the file object directly returned by urlopen, without needing to call .read() first:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Zq42-52llN",
        "colab_type": "code",
        "outputId": "dd6faeea-ae08-4c96-c220-b99e36dc107b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
        "bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "print(bs.h1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h1>An Interesting Title</h1>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_D96vXDXha",
        "colab_type": "text"
      },
      "source": [
        "# Connecting Reliably and Handling Exceptions\n",
        "The web is messy. Data is poorly formatted, websites go down, and closing tags go missing. \n",
        "Let’s take a look at the first line of our scraper, after the import statements, and figure out how to handle any exceptions this might throw:\n",
        "Two main things can go wrong in this line:\n",
        "\n",
        "•\tThe page is not found on the server (or there was an error in retrieving it).\n",
        "•\tThe server is not found.\n",
        "In the first situation, an HTTP error will be returned. This HTTP error may be “404 Page Not Found,” “500 Internal Server Error,” and so forth. In all of these cases, the urlopen function will throw the generic exception HTTPError. If an HTTP error code is returned, the program now prints the error, and does not execute the rest of the program under the else statement.\n",
        "If the server is not found at all (if, say, http://www.pythonscraping.com is down, or the URL is mistyped), urlopen will throw an URLError. This indicates that no server could be reached at all, and, because the remote server is responsible for returning HTTP status codes, an HTTPError cannot be thrown, and the more serious URLError must be caught. You can add a check to see whether this is the case:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2316OxEG2llR",
        "colab_type": "code",
        "outputId": "31b45156-d4f8-4053-bc7f-d5ac8718a4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "\n",
        "try:\n",
        "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
        "except HTTPError as e:\n",
        "    print(\"The server returned an HTTP error\")\n",
        "except URLError as e:\n",
        "    print(\"The server could not be found!\")\n",
        "else:\n",
        "    print(html.read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The server could not be found!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIj6MmS8EEiG",
        "colab_type": "text"
      },
      "source": [
        "Of course, if the page is retrieved successfully from the server, there is still the issue of the content on the page not quite being what you expected. \n",
        "\n",
        "Every time you access a tag in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually exists. \n",
        "\n",
        "If you attempt to access a tag that does not exist, BeautifulSoup will return a None object. The problem is, attempting to access a tag on a None object itself will result in an AttributeError being thrown.\n",
        "\n",
        "The following line (where nonExistentTag is a made-up tag, not the name of a real BeautifulSoup function)\n",
        "\n",
        "print(bs.nonExistentTag)\n",
        "\n",
        "returns a None object. This object is perfectly reasonable to handle and check for. \n",
        "\n",
        "The trouble comes if you don’t check for it, but instead go on and try to call another function on the None object, as illustrated in the following:\n",
        "\n",
        "print(bs.nonExistentTag.someTag)\n",
        "\n",
        "This returns an exception:\n",
        "AttributeError: 'NoneType' object has no attribute 'someTag'So how can you guard against these two situations? \n",
        "\n",
        "The easiest way is to explicitly check for both situations:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry_2OOP-2llU",
        "colab_type": "code",
        "outputId": "de142832-7b80-4133-91cc-05520f7a8b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def getTitle(url):\n",
        "    try:\n",
        "        html = urlopen(url)\n",
        "    except HTTPError as e:\n",
        "        return None\n",
        "    try:\n",
        "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
        "        title = bsObj.body.h1\n",
        "    except AttributeError as e:\n",
        "        return None\n",
        "    return title\n",
        "\n",
        "\n",
        "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
        "if title == None:\n",
        "    print(\"Title could not be found\")\n",
        "else:\n",
        "    print(title)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<h1>An Interesting Title</h1>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWQg0kpq2llZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}