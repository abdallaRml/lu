{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Step_5_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdallaRml/lu/blob/master/Copy_of_Step_5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xi3HXeBu5nc",
        "colab_type": "text"
      },
      "source": [
        "Neural Network Example\n",
        "Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow v2.\n",
        "\n",
        "This example is using a low-level approach to better understand all mechanics behind building neural networks and the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfmRZn4jvCGE",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Overview\n",
        "\n",
        "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
        "\n",
        "## MNIST Dataset Overview\n",
        "\n",
        "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. \n",
        "\n",
        "In this example, each image will be converted to float32, normalized to [0, 1] and flattened to a 1-D array of 784 features (28*28).\n",
        "\n",
        "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "In this section we will build a network that can recognize handwritten numbers.\n",
        "In order to achieve this goal, we'll use MNIST (http://yann.lecun.com/exdb/\n",
        "mnist/), a database of handwritten digits made up of a training set of 60,000\n",
        "examples, and a test set of 10,000 examples. The training examples are annotated by\n",
        "humans with the correct answer. For instance, if the handwritten digit is the number\n",
        "\"3\", then 3 is simply the label associated with that example.\n",
        "In machine learning, when a dataset with correct answers is available, we say that we\n",
        "can perform a form of supervised learning. In this case we can use training examples\n",
        "to improve our net. Testing examples also have the correct answer associated to each\n",
        "digit. In this case, however, the idea is to pretend that the label is unknown, let the\n",
        "network do the prediction, and then later on reconsider the label to evaluate how\n",
        "well our neural network has learned to recognize digits. Unsurprisingly, testing\n",
        "examples are just used to test the performance of our net.\n",
        "Each MNIST image is in grayscale and consists of 28*28 pixels. A subset of these\n",
        "images of numbers is shown in Figure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm9I7XlfUrtB",
        "colab_type": "text"
      },
      "source": [
        "One-hot encoding (OHE)\n",
        "We are going to use OHE as a simple tool to encode information used inside neural\n",
        "networks. In many applications it is convenient to transform categorical (nonnumerical)\n",
        "features into numerical variables. For instance, the categorical feature\n",
        "\"digit\" with value d in [0 â€“ 9] can be encoded into a binary vector with 10 positions,\n",
        "which always has 0 value except the d - th position where a 1 is present.\n",
        "For example, the digit 3 can be encoded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. This type of\n",
        "representation is called One-hot encoding, or sometimes simply one-hot, and is very\n",
        "common in data mining when the learning algorithm is specialized in dealing with\n",
        "numerical functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRXpKFFghRb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab82f0b5-afda-42ae-dc48-ca2e23aea6f8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inGs-vtVhXyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Intuitively, EPOCH defines how long the training should last, \n",
        "# BATCH_SIZE is the number of samples you feed in to your network at a time, \n",
        "# VALIDATION is the amount of data reserved for checking or proving the validity of the training process.\n",
        "\n",
        "# network and training\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10   # number of outputs = number of digits\n",
        "N_HIDDEN = 2048\n",
        "VALIDATION_SPLIT=0.999 # how much TRAIN is reserved for VALIDATION\n",
        "DROPOUT = 0.3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i12utq0lrVhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_summary(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Summarize current state of dataset\"\"\"\n",
        "    print('Train images shape:', X_train.shape)\n",
        "    print('Train labels shape:', y_train.shape)\n",
        "    print('Test images shape:', X_test.shape)\n",
        "    print('Test labels shape:', y_test.shape)\n",
        "    print('Train labels:', y_train)\n",
        "    print('Test labels:', y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqRoGUC_hZ1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7511fb8e-e6da-4d67-9072-1f2f91294a37"
      },
      "source": [
        "# Prepare MNIST data.\n",
        "# Next we load our dataset (MNIST, using Keras' dataset utilities), \n",
        "# and then use the function above to get some dataset metadata.\n",
        "\n",
        "# loading MNIST dataset\n",
        "# verify\n",
        "# the split between train and test is 60,000, and 10,000 respectly \n",
        "# one-hot is automatically applied\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqWcAI57rdEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "44c4586b-480f-4ec1-c795-c2d8d109213c"
      },
      "source": [
        "# Check state of dataset\n",
        "data_summary(X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images shape: (60000, 28, 28)\n",
            "Train labels shape: (60000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n",
            "Train labels: [5 0 4 ... 5 6 8]\n",
            "Test labels: [7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6XjKNWFhcK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
        "RESHAPED = 784\n",
        "\n",
        "# Flatten images to 1-D vector of 784 features (28*28).\n",
        "# To feed MNIST instances into a neural network, they need to be reshaped,\n",
        "# from a 2 dimensional image representation to a single dimension sequence. \n",
        "# We also convert our class vector to a binary matrix (using to_categorical). \n",
        "# This is accomplished below, after which the same function defined above is called again in order to show the effects of our data reshaping.\n",
        "# X_train is 60000 rows of 28x28 values --> num_features in 60000 x 784\n",
        "\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "\n",
        "# Convert to float32.\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ7ibdchheHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "816c5add-7581-4dba-c442-34d75bffb0b5"
      },
      "source": [
        "#normalize in [0,1]\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELK96vbNhjEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot\n",
        "# Note that to_categorical(Y_train, NB_CLASSES) converts the array Y_train into\n",
        "# a matrix with as many columns as there are classes. The number of rows stays the same.\n",
        "# We also convert our class vector to a binary matrix (using to_categorical). \n",
        "# Categorically encode labels\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC-w79ZWhktY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "# The Sequential class is used to define a linear stack of network layers which then, \n",
        "# collectively, constitute a model. \n",
        "# In our example below, we will use the Sequential constructor to create a model,\n",
        "# which will then have layers added to it using the add() method.\n",
        "# Both of the required data transformations have been accomplished. \n",
        "# Now it's time to build, compile, and train a neural network.\n",
        "# The code in our example uses the Sequential class.\n",
        "# It first calls the constructor, after which calls are made to the add() method to add layers to the model.\n",
        "# The first such call adds a layer of type Dense (\"Just your regular densely-connected NN layer\"). \n",
        "# The Dense layer has an output of size 16, and an input of size INPUT_DIM, which is 32 in our case.\n",
        "#  Note that only the first layer of the model requires the input dimension to be explicitly stated; \n",
        "# Note that only the first layer of the model requires the input dimension to be explicitly stated;\n",
        "#  the following layers are able to infer from the previous linear stacked layer. \n",
        "# Following standard practice, the rectified linear unit activation function is used for this layer. \n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "# The first such call adds a layer of type Dense (\"Just your regular densely-connected NN layer\").\n",
        "\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "   \t\tinput_shape=(RESHAPED,),\n",
        "   \t\tname='dense_layer', activation='relu'))\n",
        "#Now our baseline is 90.81% on the training set, 91.40% on validation, and 91.18%\n",
        "#on test. A second improvement is very simple. We decide to randomly drop â€“ with\n",
        "#the DROPOUT probability â€“ some of the values propagated inside our internal dense\n",
        "#network of hidden layers during training. In machine learning this is a well-known\n",
        "#form of regularization. Surprisingly enough, this idea of randomly dropping a few\n",
        "#values can improve our performance. The idea behind this improvement is that\n",
        "#random dropout forces the network to learn redundant patterns that are useful\n",
        "#for better generalization:\n",
        "\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "\n",
        "# An initial improvement is to add additional layers to our network because these\n",
        "# additional neurons might intuitively help it to learn more complex patterns in the\n",
        "# training data. In other words, additional layers add more parameters, potentially\n",
        "# allowing a model to memorize more complex patterns. \n",
        "#So, after the input layer, we have a first dense layer with N_HIDDEN neurons and an activation function \"ReLU.\"\n",
        "# This additional layer is considered hidden because it is not directly connected either\n",
        "# with the input or with the output. After the first hidden layer, we have a second\n",
        "# hidden layer again with N_HIDDEN neurons followed by an output layer with 10\n",
        "# neurons, each one of which will fire when the relative digit is recognized. The\n",
        "# following code defines this new network:\n",
        "\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "   \t\tname='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "# The final layer is a single neuron with activation function \"softmax\", which is a\n",
        "# generalization of the sigmoid function. As discussed earlier, a sigmoid function\n",
        "# output is in the range (0, 1) when the input varies in the range (âˆ’âˆž, âˆž) . Similarly,\n",
        "# a softmax \"squashes\" a K-dimensional vector of arbitrary real values into a\n",
        "# K-dimensional vector of real values in the range (0, 1), so that they all add up to 1.\n",
        "# In our case, it aggregates 10 answers provided by the previous layer with 10 neurons.\n",
        "# What we have just described is implemented with the following code:\n",
        "\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "   \t\tname='dense_layer_3', activation='softmax'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwIiCgK5hm3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "085a1bc6-33b7-4b6c-cdfd-b046c597ddde"
      },
      "source": [
        "# summary of the model\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 2048)              1607680   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 5,824,522\n",
            "Trainable params: 5,824,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrwFU8cNh9-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "# Once we define the model, we have to compile it so that it can be executed by\n",
        "# TensorFlow 2.0. There are a few choices to be made during compilation. \n",
        "# Firstly, we need to select an optimizer, which is the specific algorithm used to update\n",
        "# weights while we train our model.\n",
        "# Second, we need to select an objective function, which is used by the optimizer to navigate the space of weights (frequently,\n",
        "# objective functions are called either loss functions or cost functions and the process of\n",
        "# optimization is defined as a process of loss minimization). \n",
        "# Third, we need to evaluate the trained model.\n",
        "\n",
        "#Some common choices for objective functions are:\n",
        "#â€¢ MSE, which defines the mean squared error between the predictions and the true values.\n",
        "#â€¢ binary_crossentropy, which defines the binary logarithmic loss.\n",
        "#â€¢ categorical_crossentropy, which defines the multiclass logarithmic loss.\n",
        "\n",
        "# Some common choices for metrics are:\n",
        "# â€¢ Accuracy, which defines the proportion of correct predictions with respect to the targets\n",
        "# â€¢ Precision, which defines how many selected items are relevant for a multilabel classification\n",
        "# â€¢ Recall, which defines how many selected items are relevant for a multi-label classification\n",
        "\n",
        "# With both the training data defined and model defined, it's time configure the learning process. \n",
        "# This is accomplished with a call to the compile() method of the Sequential model class. \n",
        "# Compilation requires 3 arguments: an optimizer, a loss function, and a list of metrics.\n",
        "# In our example, set up as a multi-class classification problem, we will use the Adam optimizer,\n",
        "#  the categorical crossentropy loss function, and include solely the accuracy metric.\n",
        "\n",
        "# optimization algorithm used to reduce the mistakes made by neural networks after each training epoch.\n",
        "# TensorFlow implements a fast variant of gradient descent known as SGD and many\n",
        "# more advanced optimization techniques such as RMSProp and Adam. RMSProp\n",
        "# and Adam include the concept of momentum\n",
        "\n",
        "model.compile(optimizer='Adam', \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# The with the call made to compile() with these arguments, our model now has its learning process configured."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWUpnAdiBUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "49d50b76-3815-4991-ecc0-7b708f376a99"
      },
      "source": [
        "#training the moodel\n",
        "# At this point we have training data and a fully configured neural network to train with said data. \n",
        "# All that is left is to pass the data to the model for the training process to commence, a process which is completed by iterating on the training data. \n",
        "# Training begins by calling the fit() method.\n",
        "# At minimum, fit() requires 2 arguments: input and target tensors. \n",
        "# If nothing more is provided, a single iteration of the training data is performed, which generally won't do you any good. \n",
        "# Therefore, it would be more conventional to, at a practical minimum, \n",
        "# define a pair of additional arguments: batch_size and epochs. Our example includes these 4 total arguments.\n",
        "\n",
        "#Once the model is compiled, it can then be trained with the fit() method, which specifies a few parameters:\n",
        "# â€¢ epochs is the number of times the model is exposed to the training set. At\n",
        "# each iteration the optimizer tries to adjust the weights so that the objective function is minimized.\n",
        "# â€¢ batch_size is the number of training instances observed before the\n",
        "# optimizer performs a weight update; there are usually many batches per epoch.\n",
        "# Training a model in TensorFlow 2.0 is very simple:\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "\n",
        "# Note that the epoch accuracies are not particularly admirable, which makes sense given the random data which was used."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 12s 12s/step - loss: 2.2980 - accuracy: 0.1167 - val_loss: 1.9776 - val_accuracy: 0.3753\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 12s 12s/step - loss: 1.5285 - accuracy: 0.5833 - val_loss: 1.5671 - val_accuracy: 0.5398\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.9251 - accuracy: 0.8833 - val_loss: 1.2367 - val_accuracy: 0.6471\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.4990 - accuracy: 0.9500 - val_loss: 1.0715 - val_accuracy: 0.6681\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.2260 - accuracy: 0.9833 - val_loss: 1.0258 - val_accuracy: 0.6672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbdb2544438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreoBcvrhpBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "383db473-7d09-451a-f2a1-eef65ee4c7cc"
      },
      "source": [
        "#evalute the model\n",
        "# Once the model is trained, we can evaluate it on the test set that contains new\n",
        "# examples never seen by the model during the training phase.\n",
        "# Note that, of course, the training set and the test set are rigorously separated. There\n",
        "# is no point evaluating a model on an example that was already used for training.\n",
        "# In TensorFlow 2.0 we can use the method evaluate(X_test, Y_test) to compute the\n",
        "# test_loss and the test_acc:\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "print('Test loss:', test_loss)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 11ms/step - loss: 1.0234 - accuracy: 0.6654\n",
            "Test accuracy: 0.6654000282287598\n",
            "Test loss: 1.0234118700027466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5CfPrmchq9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making prediction\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVbE7mAB6jFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e03730d2-7c73-4c7e-eeec-9f3fd0a5e358"
      },
      "source": [
        "# Summary of neural network\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 2048)              1607680   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_layer_2 (Dense)        (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_layer_3 (Dense)        (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 5,824,522\n",
            "Trainable params: 5,824,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}